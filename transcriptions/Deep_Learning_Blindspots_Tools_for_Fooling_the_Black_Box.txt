Deep Learning Blindspots
Tools for Fooling the "Black Box"
Katharine Jarmul, 2017

https://media.ccc.de/v/34c3-8860-deep_learning_blindspots


Result confidence: 0.908893883228302
And I will let Catherine take the stage now. Awesome. Well, thank you so much for the introduction and thank you so much for being here taking your time. I know that Congress is really exciting. So I really appreciate you spending some time with me today. It's my first ever Congress. So I'm also really excited and I want to meet new people. So if you want to come say hi to me later, I'm somewhat friendly so we can maybe be friends later today. What we're going to talk about is deep learning blind spots or how to full artificial intelligence. I like to put artificial intelligence in quotes because yeah, we'll talk about that. But I think it should be in quotes. And today we're going to talk a little bit about deep learning how it works and how you can
Result confidence: 0.8406584858894348
Maybe full it. So I asked us is AI becoming more intelligent.
Result confidence: 0.8908969759941101
And I ask this because when I open a browser and of course often, it's Chrome and Google is already prompting me for what I should look at and it knows that I work with machine learning, right and these are the headlines that I see every day our computers already smarter than humans.
Result confidence: 0.9068611264228821
If so, I think we could just pack up and go home. Right like we fix computers, right if computer is smarter than me that I already fixed it. We can go home. There's no need to talk about computers anymore. Let's just move on with life. But but that's not true. Right we know because we work with computers and we know how stupid computers are. Sometimes they're pretty bad computers do only what we tell them to do generally so I don't think a computer can think and be smarter than me. So with the same types of headlines that you see this then you also see this
Result confidence: 0.8956703543663025
And yeah, so Apple recently released their faith ID and this unlocks your phone with your faith. And it seems like a great idea right you have a unique Faith you have a face. Nobody else can take your faith. But unfortunately what we find out about computers is that they're awful sometimes and for these women for this Chinese woman that owned an iPhone her co-worker was able to unlock her phone.
Result confidence: 0.901178777217865
And I think Hendrick and Karen talked about if you were here for the last talk. We have a lot of problems in machine learning and one of them is stereotypes and Prejudice that are within our training data or within our minds that leak into our models and perhaps they didn't do adequate training data on determining different features of Chinese folks and perhaps it's other problems with their model or their training data or whatever they're trying to do but they clearly have some issues right? So when somebody asked me is a I going to take over the world and is there a Super Robot that's going to come and be my new, you know leader or so to speak. I tell them we can't even figure out the stuff that we already have in production. So if we can't even figure out the stuff we already have in production. I'm a little bit less worried than of the Super Robot coming to kill me.
Result confidence: 0.9059745073318481
That said unfortunately the powers that be the powers that be a lot of times they believe in this and they believed strongly in artificial intelligence and machine learning and they're collecting data every day about you and me and everyone else and they're going to use this data to build even better models and this is because the revolution that we're seeing now in machine learning has really not much to do with new algorithms or architectures. It has a lot more to do with heavy compute and with massive massive data sets and the more that we have training data of petabytes per 24 hours or even left the more we're able to essentially fix up the parts that don't work. So well and the companies that we see here are companies that are investing heavily in machine learning and Ai and part of how they're investing heavily.
Result confidence: 0.8838316202163696
Lee is there collecting more and more data about you and me and everyone else Google and Facebook more than 1 billion active users. I was surprised to know that in Germany the desktop search traffic for Google is higher than most of the rest of the world.
Result confidence: 0.9049850106239319
And for by do they're growing with the speed that Broadband is available. And so what we see is these people are collecting this data and they also are using new technologies like gpus and TP use in new ways to parallelize workflows. And with this they're able to mess up left, right? They're still messing up at they mess up slightly left and they're not going to get on interested in this topic. So we need to kind of start to prepare how we respond to this type of behavior. And so one of the things that has been a big area of research actually also for a lot of these companies is what we'll talk about today and that's adversarial machine learning. But the first thing that we'll start with is what is behind what we call a i
Result confidence: 0.9065424799919128
So most of the time when you think of AI or something like Theory and so forth you are actually potentially talking about what an old-school rule based system. This is a rule like you say a particular thing and then series like yes, I know how to respond to this and we even hard program these types of things in right that is one version of AI is essentially it's been pre-programmed to do and understand certain things another form that usually like, for example for the people that are trying to build AI robots and the people that are trying to build what we call General AI. So this is something that can maybe learn like a human they'll use reinforcement learning. I don't specialize in reinforcement learning but what it does is it essentially tries to reward you for behavior that you're expected to do. So if you complete a task you get a cookie you complete to other tasks you get two or three more cookies depending on how important the task is.
Result confidence: 0.9052163362503052
And this will help you learn how to behave to get more points and it's used a lot in robots and gaming and so forth and I'm not really going to talk about that today because most of that is still not really something that you are I interact with what I am going to talk about today is neural networks, or as some people like to call them deep learning, right? So deep learning one the neural network versus deep learning battle a while ago. So here's an example neural network. We have an input layer and that's where we essentially make a quantitative version of whatever our data is. So we need to make it into numbers then we have a hidden layer and we might have multiple hidden layers and depending on how deep our network is or network inside a network, right which is possible. We might have very much different layers there and they may even act encyclical waste and then that's where all the weights and the variables and the Learning Happens. So that has holds a lot of
Result confidence: 0.9057846069335938
Then data that we eventually want to train their and finally we have an output layer and depending on the network and what we're trying to do the output layer can vary between something that looks like the input like for example, if we want to machine translate then I want the output to look like the input right? I wanted to just be in a different language or the output could be a different class. It can be you know, this is a car or this is you know a train and so forth. So it really depends what you're trying to solve but the output layer gives us the answer and how we train this is we use back propagation and bought back propagation is nothing new and neither is one of the most popular methods to do so which is called stochastic gradient descent and what we do when we go through that part of the training is we go from the output layer we go backwards through the network. That's why it's called back propagation great. And as we go backwards through the network, we upvote and in the most simple way, we upvote and downvote what's working and what's not working.
Result confidence: 0.9014288783073425
So we say oh you got it, right you get a little bit more important. So you got it wrong you get a little bit less importance. And eventually we hope over time that they essentially correct each other's errors enough that we get a right answer. So that's a very general overview of how it works. And the cool thing is is because it works that way we can fool it and people have been researching ways to fool it for quite some time. So give you a brief overview of the history of this field so we can kind of know where we're working from and maybe hopefully then we're going to in 2005 was one of the first most important papers to approach adversarial learning and it was written by a series of researchers and they wanted to see if they could act as an informed attacker and attack a linear classifier. So this is just a spam filter and they're like, can I send spam to my friends? I don't know why they wanted to do this, but can I send spam?
Result confidence: 0.8951922655105591
To my friend if I tried testing out a few ideas and what they were able to show is yes rather than just, you know, trial and error which anybody can do or Brute Force attack of just like send a thousand emails and see what happens. They were able to craft a few algorithms that they could use to try and find important words to change to make it go through the spam filter.
Result confidence: 0.9003750085830688
In 2007 nips, which is a very popular machine learning conference had one of their first all day workshops on computer security and when they did so they had a bunch of different people that were working on machine learning in computer security from malware detection to network intrusion detection to of course fam and they also had a few talks on this type of adversarial learning. So how do you act as an adversary to your own model? And then how do you learn how to counter that adversary?
Result confidence: 0.8992781043052673
In 2013, there was a really great paper that got a lot of people's attention called poisoning attacks against support Vector machines now support Vector machines are essentially usually a linear classifier and we use them a lot to say this is member of this class that or another when we pertain to text so I have text and I want to know what the text is about or I want to know if it's a positive or negative sentiment a lot of times. I'll use a support Vector machine and we call them svm's as well and Batista Biggio was the main researcher and he's actually written quite a lot about these poisoning attacks and he poisoned the training data. So for a lot of this these systems, sometimes they have active learning and this means you are I when we classify our emails as spam, we're helping train the network and so he poisoned the training data and was able to show that by poisoning it in a particular way that he
Result confidence: 0.9071923494338989
Able to then send spam email because he knew what words were then benign essentially he went on to study of few other things about biometric data if you're interested in Biometrics, but then in twenty thousand fourteen Christian says get e in good fellow and a few other main researchers at Google brain released entry go intriguing properties of neural networks, and that really became the explosion of what we're seeing today in adversarial learning and what they were able to do is they were able to say we believe there's linear properties of these neural networks, even if they're not necessarily losing your network and we believe we can exploit them to fool them and they first introduced then the fast gradient sign method, which we'll talk about later today.
Result confidence: 0.8825649619102478
So how does it work first? I want us to get a little bit of an intuition around how this works.
Result confidence: 0.9008904695510864
Here's a graphic graphic of gradient descent and in gradient descent we have this vertical access is our cost function. And what we're trying to do is we're trying to minimize cost we want to minimize the error and so when we start out we just chose random weights and variables. So all of our hidden layers, they just have Maybe random weights or random distribution and then we want to get to a place where the weights have meaning, right? We want our Network to know something even if it's just a mathematical pattern, right? So we start in the high area of the graph or the reddish area and that's where we started we have high error there and then we try to get to the lowest area of the graph or hear the dark blue that is right about here.
Result confidence: 0.9067902565002441
But sometimes what happens and so as we learn as we go through epics and training we're moving slowly down and hopefully we're optimizing but we might end up in instead of This Global minimum. We might end up in the local minimum, which is the other Trail and that's fine because it's still zero error, right? So we're still probably going to be able to succeed but we might not get the best answer all the time. What adversarial tries to do in the most basic of ways it is is essentially tries to push the error rate back up the hill for as many units as it can so it essentially tries to increase the error slowly through perturbations and by disrupting let's say the weakest links like the one that did not find the global minimum but instead found a local minimum we can hopefully full the network because we're finding those weak spots.
Result confidence: 0.8805286288261414
We're capitalizing on them essentially.
Result confidence: 0.8875294327735901
So what is an adversarial example actually look like you may have already seen this because this is very popular on the Twitter sphere and a few other places. But this is a series of researchers at MIT and it was debated whether you could do adverse adversarial learning in the real world. A lot of the research has just been a still image and what they were able to show is they created a 3D printed Turtle. I mean, it looks like a turtle to you as well, correct?
Result confidence: 0.9085685014724731
And this 3D printed turtle by the Inception Network, which is a very popular computer vision network is a rifle and it is a rifle in every angle that you can see and the way they were able to do this and I don't know the next time it goes around as you can see perhaps and it's a little bit easier on the video, which I have posted all shared the and you can see perhaps the there's a slight discoloration of the shell and they messed with the texture and my messing with this texture and the colors they were able to fool the neural network. They were able to activate different neurons that were not supposed to be activated units I should say and so what we see here is yeah, it can be done in the real world. And when I saw this I started getting really excited because video surveillance is a real thing, right? So if we can start fooling 3D objects, we can perhaps start fooling other things in the
Result confidence: 0.7766821980476379
A world that we would like to fool.
Result confidence: 0.9049802422523499
So why do adversarial examples exist we're going to talk a little bit about some things that are approximations of what's actually happening. So, please forgive me for not being always exact but I would rather us all have a general understanding of what's Happening across the top row. We have an input layer and these images to the left we can see are the source images and the source image is like a piece of farming equipment or something. And on the right we have our guide image. This is what we're trying to get the network to see we wanted to miss miss classify this farm equipment as a pink bird. So what these researchers did is they targeted different layers of the network and they said okay we're going to use this method to Target this particular layer and we'll see what happens and so as they targeted these different layers you can see what's happening on the internal visualization now neural networks can't see right there.
Result confidence: 0.8993189930915833
King add matrices of numbers but what we can do is we can use those internal values to try and see with our human eyes what they are learning and we can see here clearly inside the network. We no longer see the farming equipment, right? We see a pink Bert and this is not visible to our human eyes. Now if you really study and if you enlarge the image, you can start to see okay, there's a little bit of pink here or greens. I don't know what's happening, but we can still see it in the neural network. We have tricked now people don't exactly know yet why these these blind spots exist. So it's still a area of active research exactly why we can full neural network so easily there are some prominent researchers that believe that neural networks are essentially very linear.
Result confidence: 0.898825466632843
And that we can use this simple linearity to misclassified to jump into another area. But there are others that believe that there's these pockets are blind spots and that we can then find these blind spots where these neurons really are the weakest links and they maybe even haven't learned anything and if we change their activation, then we can fool the network easily. So this is still an area of active research and let's say you're looking for your thesis. This would be a pretty neat thing to work on.
Result confidence: 0.9054164290428162
So we'll get into just a brief overview of some of the math behind the most popular methods first. We has a fast gradient sign method and that is was used in the initial paper and now there's been many iterations on it. And what we do is we have our same cost function. This is so this is the same way that we're trying to train our Network and is trying to learn and we take the gradient sign of that and if you can think it's okay, if you're not used to doing Vector calculus and especially not with a pen and paper in front of you but what you think we're doing is we're essentially trying to calculate some approximation of a derivative of the function and this can kind of tell us where is it going and if we know where it's going we can maybe anticipate that and change and then for to create the adversarial images we then take the original input plus a small number Epsilon.
Result confidence: 0.9010477662086487
Times that gradient sign for the Jacobian fail is he map this is a newer method and it's a little bit more effective but it takes a little bit more compute. And so this Jacobian salience he map uses a Jacobian matrix and if you remember also and it's okay, if you don't a Jacobian matrix will look at the four derivative of a function. So you take the for derivative of a cost function and it gives you a matrix at that at that vector and it gives you a matrix that is a point wise approximation if the function is differentiable at that input Vector. Don't worry, you can review this later too. But the Jacobian matrix then we used to create this alien see map the same way where we're trying to essentially find some sort of linear approximation or point-wise approximation. And we then want to find two pixels that we can perturb that cause the most disruption and
Result confidence: 0.8942975401878357
Then we continued to the next and unfortunately, this is currently a no N squared problem. But there's a few people that are trying to essentially find ways that we can approximate this and make it faster. So maybe now you want full a network to and I hope you do because that's what we're going to talk about.
Result confidence: 0.9065086841583252
First you need to pick a problem or a network type. So you may already know but you may want to investigate what perhaps is this company using what perhaps is this method using and do a little bit of research because that's going to help you. Then you want to research state-of-the-art methods and this is like a typical research statement that you have a new state-of-the-art method, but the good news is that the state-of-the-art two to three years ago is most likely in production or in systems today. So once they find ways to speed it up, they usually some approximation of that is deployed and a lot of times these are then publicly available models. So a lot of times if you're already working with a deep learning framework, they'll come prepackaged was a few of the different popular models so you can even use that if you're already building neural networks, of course, you can build your own.
Result confidence: 0.9046177268028259
An optional step but one that might be recommended is to find tuner model and what this means is to essentially take a new training data set may be data that you think this company is using or that you think this network is using and you're going to remove the last few layers of the neural network and you're going to retrain it. So you essentially are nicely piggybacking on the work of the pre-trained model and you're using the final layers to create finesse. This is essentially me makes your model better at the task that you have for it. Finally then you use a library and we'll go through a few of them. But some of the ones that I have used myself is clever Hans Deep full and deep owning and these all come with nice built-in features for you to use for let's say the fast gradient sign method the Jacobian saliency map and a few other methods that are available. Finally. It's not going to always work. So depending on your
Result confidence: 0.8893022537231445
Or thin your target you won't always necessarily find a match what researchers have shown as it's a lot easier to fool a network that a cat is a dog than it is to full Network that a cat is an airplane. And this is just like we can make these intuitive. So you might want to take an input that's not super dissimilar from where you want to go. But is this similar enough and you want to test it locally and then finally test the ones with the highest Miss classification rates on the target Network.
Result confidence: 0.9028347730636597
And you might say Catherine or you can call me Kay Jam. That's okay. You might say I don't know what the person is using. I don't know what the company is using and I will say it's okay because what's been proven is you can attack a black box model. You do not have to know what they're using. You do not have to know exactly how it works. You don't even have to know their training data because what you can do is if it has okay addendum, it has to have some API you can interface with but if it has an API, you can interface with or even any API you can interact with that uses the same type of learning you can collect training data by querying the API.
Result confidence: 0.8946437835693359
And then your training your local model on that data that you're collecting. So you're collecting the data you're training your local model. And as your local model gets more accurate and more similar to the deployed black box that you don't know how it works. You are then still able to fool it and what this paper prove Nicholas Papa not and a few other great researchers is that with usually less than 6,000 queries. They were able to fool the network between 84 and 97 percent certainty.
Result confidence: 0.9027436971664429
And what the same group of researchers also studied is the ability to transfer the ability to fool wanton Network into another Network and they call that transferability so I can take a certain type of network and I can use adversarial examples against this network to fool a different type of machine learning technique. And here we have their Matrix there heat map that shows us exactly what they were able to fool. So we have across the left hand side here the source machine learning Technique. We have deep learning logistic regression svm's like we talked about decision trees and K nearest neighbors and across the bottom. We have the target machine learning. So what were they targeting? They created the adversaries with the left hand side and they targeted across the bottom. We finally have an ensemble model at the end.
Result confidence: 0.8765261173248291
And what they were able to show is like for example as sion's and decision trees are quite easy to fool.
Result confidence: 0.9019060730934143
But logistic regression a little bit less so but still strong for deep learning and K nearest neighbors if you train a deep learning model or a k-nearest neighbor model, then that performed fairly well against itself and so what they're able to show is that you don't necessarily need to know the target machine and you don't even have to get it right. Even if you do know you can use a different type of machine learning technique to Target the network.
Result confidence: 0.9072201251983643
So we'll look at six lines of python here and in the six lines of python, I'm using the clever Hans library and in six lines of python. I can both generate my adversarial imput and I can even predict on it. So if you don't code python, it's pretty easy to learn and pick up and for example here we have carrots and Karis is a very popular deep learning library in Python. It usually works with a say, ah, no or tensorflow back end and we can just wrap our model pass it to the fast gradient method class laughs and then set up some parameters. So here's our Epsilon and a few extra parameters. This is to tune our adversary and finally we can generate our adversarial examples and then predict on them. So in a very small
Result confidence: 0.8981389999389648
Mount of python were able to Target and trick Network and if you're already using tensorflow or caref it already works with those libraries deep pwning is one of the first libraries that are heard about in the space and it was presented at def Con in 2016. And what it comes with is a month of tensorflow built-in coat it even comes with a way that you can train the model yourself. So it has a few different models of few different convolutional neural networks, and these are predominantly used in computer vision it also however has a semantic model and I normally work in NLP and I was pretty excited to try it out.
Result confidence: 0.8917291164398193
And what it comes built with is the Rotten Tomatoes sentiment. So this is Rotten Tomatoes movie reviews that try to learn is it positive or negative? So the original text that I input in when I was generating my adversarial networks was more trifle than triumph, which is Real review and adversarial text editor gave me was Jonah refreshing hauntingly key.
Result confidence: 0.9085822701454163
Yeah, so I was able to fool my network but I lost any type of meaning and this is really the problem when we think about how we apply adversarial learning to different tasks is it's easy for an image if we make a few changes for it to retain its image, right? It's many many pixels, but when we start going into language if we change one word and then another word and another word or maybe we change all of the words. We no longer understand as humans and I would say this is garbage in garbage out. This is not actual adversarial learning. So we have a long way to go when it comes to language tasks and being able to do adversarial learning and there is some research in this but it's not really advanced yet. So hopefully this is something that we can continue to work on and Advance further and if so, we need to support a few different types of networks that are more common in NLP than they are in computer vision.
Result confidence: 0.8980077505111694
There's some other notable open source libraries that are available to you and I will cover just a few here. There's the Vanderbilt computational economics research lab that has ad lib. And this allows you to do poisoning attacks. So if you want to Target Training data and poison it then you can do so with that and uses scikit-learn Deep full allows you to do the fast gradient sign method, but it tries to do smaller perturbations. It tries to be less detectable to us humans.
Result confidence: 0.9049338102340698
Is based on theano, which is another library that I believe uses Lua as well as python full box is kind of neat because I only heard about it last week but it collects a bunch of different techniques all in one library and you can use it with 1 Interfaith. So if you want to experiment with a few different ones at once, I would recommend taking a look at that and finally for something that will talk about briefly in a short period of time. We have evolving AI lab which release of fooling library and this fooling library is able to generate images that you are I can't tell what it is, but that the neural network is convinced. It is something so this will talk about maybe some applications of this in a moment, but they also open sourced all their code and their researchers who open source of code, which is always very exciting as you may have known from some of the recent research. I already cited.
Result confidence: 0.9036368131637573
Most of the studies and the research in this area has been on malicious attacks. So there's very few people trying to figure out how to do this for what I would call benevolent purposes. Most of them are trying to act as an adversary in the traditional computer security sense there perhaps studying spam filters and how spammers can get by them there perhaps looking at Network intrusion or botnet attacks and so forth there perhaps looking at self-driving cars. So and I know that was referenced earlier as well at Hendrick and Karen's talk the perhaps trying to make a yield sign look like a stop sign or a stop sign look like a yield sign or speed limit and so forth and scarily they are quite successful at this.
Result confidence: 0.907335638999939
Or perhaps it looking at data poisoning. So how do we poison the model? So we render it useless in a particular context so we can utilize that and finally for malware. So what if a few researchers were able to show is by just changing a few things in the malware they were able to upload their malware to Google mail and send it to someone and this is still fully functional malware in that same sense. There's amalgam project which uses a generative adversarial Network to create malware that works I guess. So there's a lot of research of these kind of malicious attacks within adversarial learning.
Result confidence: 0.8978268504142761
But what I wonder is how might we use this for good and I feel good in quotation marks because we all have different ethical and moral systems. We use and what you may decide is ethical for you might be different but I think as a community especially at a conference like this. Hopefully we can Converge on some ethical privacy concerned version of using these Networks.
Result confidence: 0.8574334979057312
So I've composed a few ideas and I hope that this is just a starting list of a longer conversation.
Result confidence: 0.9084587097167969
One idea is that we can perhaps use this type of adversarial learning to full surveillance. So as surveillance affects you and I it even disproportionately affects people that most likely can't be here. And so whether or not we're personally affected we can care about the many lives that are affected by this type of surveillance and we can try and build ways to full surveillance systems stenography so we could potentially in a world where more and more people have less of a private we have sending messages to one another we can perhaps use adversarial learning to send private messages add were fooling. So again where I might have quite a lot of privilege and I don't actually see ads that are predatory on me as much there's a lot of people in the world that Faith predatory advertising and so, how can we help those problems?
Result confidence: 0.8971425294876099
Developing adversarial techniques poisoning your own private data this depends on whether you actually need to use the service and whether you like how the service is helping you with the machine learning, but if you don't care or if you need to essentially have a burn box of your data, then potentially you could poison your own private data. And finally, I want us to use it to investigate deployed models. So even if we don't actually need a youth for fooling this particular Network the more we know about what's deployed and how we can fool it the more we're able to keep up with this technology is it continues to evolve? So the more that we're practicing the more that we're ready for whatever might happen next.
Result confidence: 0.889385998249054
And finally, I really want to hear your ideas as well. So I'll be here throughout the whole Congress. And of course you can share during the Q&A time. If you have great ideas. I really want to hear them.
Result confidence: 0.9088848829269409
So I decided to play around a little bit with some of my ideas and I was convinced perhaps that I could make Facebook think I was a cat. This is my goal. Ken Facebook think I'm a cat because nobody really likes Facebook. I mean, let's be honest, right but I have to be on it because my mom messages me there and she doesn't use email anymore. So I'm on Facebook anyway, so I used a pre-trained Inception model in Karis and I fine-tune the layers and I'm not a computer vision person really but it took me like a day of figuring out how computer vision people transfer their data into something. I can put inside of a network figure that out and I was able to quickly train a model and the model could only distinguish between people and cats. That's all the modern knew how to do I give it a picture. It says it's a person or it's a cat. I have no idea. I actually didn't try just giving
Result confidence: 0.9033498167991638
Image of something else it would probably guess it's a person or a cat maybe 50/50. Who knows and what I did was I use an image of myself and eventually I had my fast gradient sign method. I use clever Hans and I was able to slowly increase the Epsilon and so the Epsilon as it slow you you and I can't see the perturbations that also the network can't see the perturbations. So we need to increase it. And of course as we increase it when we're using a technique like ft SM we are also increasing the noise that we see and when I got to point to one Epsilon and I kept uploading it to Facebook and Facebook have saying yeah, do you want to tag yourself? And I'm like, no. I don't I'm just testing finally. I got to point to an Epsilon and Facebook no longer knew I was a faith. So I was just a book as a cat book Maybe.
Result confidence: 0.8987451195716858
So unfortunately as we see I didn't actually become a cat because that would be pretty neat. But I did I was able to fool it. I spoke with a computer vision specialist that I know and she actually worked from this and I was like what methods do you think Facebook is using like that? I really full the neural network or what did I do and she's convinced most likely that they're actually using a statistical method called Viola Jones which takes a look at the statistical distribution of your face and tries to guess if there's really a face there, but what I was able to show transferability is that I can use my neural network even to fool this statistical model. So now I have a very noisy, but happy photo on Facebook.
Result confidence: 0.9045327305793762
Another youth case potentially is adversarial stenography and I was really excited reading this paper what this paper covered and they actually release the library as I mentioned is they studied the ability of a neural network to be convinced that something's there. That's not actually there and what they use is the nip training set. I'm sorry, if that's like a trigger word. If if you've used mnist a million times then I'm sorry for this but with uses mnist, which is zero through nine of digits and what they were able to show using evolutionary network is they were able to generate things that to us look maybe like art and they actually used it on the thief our data set to which has colors and it was quite beautiful some of what they created. In fact, they showed it in a gallery and what the network thiis here is the digits across the top they see that digit they are
Result confidence: 0.9073715209960938
More than 99 percent convinced that that digit is there and what we see is pretty patterns or just noise and when I was reading this paper, I was thinking how can we use this to send messages to each other that nobody else will know is there I'm just sending really nice. I'm an artist in this my art and I'm sharing it with my friend and in a world where I'm afraid to go home because there's a crazy person in charge and I'm afraid that they might look at my phone and my computer and a million other things and I just want to make sure that my friend has my PIN number or this or that or whatever I see use case for my life. But again, I leave fairly privileged privileged life. There are other people were there actual life and livelihood and security might depend on using a technique like this and I think we could use adversarial learning to create a new form of
Result confidence: 0.629551887512207
an overview
Result confidence: 0.8945886492729187
finally, I cannot impress enough that the more information we have about the systems that we interact with every day that our machine learning systems that are AI systems or whatever you want to call it that are deep networks. The more information. We have the better we can fight them. Right? We don't need perfect knowledge, but the more knowledge that we have the better and adversary we can be
Result confidence: 0.905472993850708
and if I thankfully now live in Germany and if you are also European resident we have gdpr which is the general data protection regulation and it goes into effect in May of 2018 and we can use gdpr to make requests about our data we can use gdpr to make requests about machine learning systems that we interact with. This is a right that we have and in recital 71 of the GDP are is States. The data subject should have the right to not be subject to a decision which may include a measure of evaluating personal aspects related to him or her which is based solely on automated processing and which produces legal effects concerning him or her or similarly significant effects him or her such as automatic refusal of an online credit application or E recruiting practices without any human intervention, and I'm not a lawyer.
Result confidence: 0.9021866321563721
And I don't know how this will be implemented and it's a recital so we don't even know if it will be enforced the same way, but the good news is pieces of this same sentiment are in the actual amendments and if they're in the Amendments, then we can legally use them and what it also says is we can ask companies to Port our data other places. We can ask companies to delete our data we can ask for information about a how our data is processed. We can ask for information about what different automated decisions are being made and the more we all here ask for that data the more we can also share that same information with people worldwide because you know the systems that we interact with they're not special to us. They're the same types of systems that are being deployed everywhere in the world so we can help our fellow humans outside of Europe by being good caretakers and using our rights to make more.
Result confidence: 0.8743926286697388
Available to the entire world and to use this information to find ways to use adversarial learning to fool these types of systems.
Result confidence: 0.907024085521698
So how else might we be able to harness it for good. I cannot focus enough on GDP are and our right to collect more information about the information. They're already collecting about us and everyone else. So use it. Let's find ways to share the information we gain from it. So I don't want it to just be that one person requested and they learned something we have to find ways to share this information with one another Tess low-tech ways. So I'm big into you know, I'm so excited about the maker space here and maker culture and other low Tech or human crafted wasteful networks. We can use adversarial learning perhaps to get good ideas on how to full networks to get lower Tech ways. What if I painted red pickled pixels all over my face would I still be recognized what I not? Let's experiment with things that
Result confidence: 0.8385912775993347
He learned from adversarial learning and try to find other lower Tech solutions to the same problem.
Result confidence: 0.9073343276977539
Finally or nearly finally we need to increase the research Beyond just computer vision quite a lot of adversarial learning has been only in computer vision and while I think that's important, and it's also been very practical because we can start to see how we can fool something. We need to figure out natural language processing. We need to figure out other ways that machine learning systems are being used and we need to come up with clever ways to fool them finally spread the word, so I don't want the conversation to end here. I don't want the conversation to end at Congress. I want you to go back to your hacker Collective your local CCC. The people that you talk with your co-workers, and I want you to spread the word. I want you to do workshops on everything you're learning. I want more people to not treat this AI as something mystical and Powerful because unfortunately, it is powerful, but it's not mystical. So we need to demystify the space. We need to experiment we need to hack on it.
Result confidence: 0.8745859265327454
And we need to find ways to play with it and spread the word to other people.
Result confidence: 0.8278234004974365
Finally, I really want to hear your other ideas.
Result confidence: 0.9087656140327454
And before I leave today, I have to say a little bit about why I decided to join the resiliency track this year. I read about the resiliency track and I was really excited it spoke to me and I said I want to live in a world where even if there's an entire burning trash fire around me. I know that there are other people that I care about that I can count on but I can work with to try and at least protect portions of our world to try and protect ourselves to try and Pull It protect people that do not have as much privilege. So what I want to be a part of is something that can use maybe the skills. I have the skills you have to do something with that and your data is a big source of value for everyone any free service you use they are selling your data. Okay. I don't know that for a fact but it is very 30.
Result confidence: 0.9045568704605103
I feel very certain about the fact that they're most likely selling your data. And if they're selling your data, they might also be buying your data and there is a whole Market that's legal. That's freely available to buy and sell your data and they make money off of that and they mined more information and make more money off of that and so forth. So how we read a little bit of my opinions that I put forth on this determine who you share your data with and for what reasons gdpr and data portability give us European residents stronger rights than most of the world. Let's use them. Let's choose privacy concerned ethical data companies over corporations that are entirely built on selling ads. Let's build startups organizations open source tools and systems that we can be truly proud of and let's support our data to those.
Result confidence: 0.9088948965072632
We have us you have time for a few questions. I'm not done yet. It's fine. No big deal. So machine learning closing remarks brief brief Round Up closing Reuters that machine learning is not very intelligent. I think artificial intelligence is a misnomer in a lot of ways, but this doesn't mean that people are going to stop using it. In fact, there's very smart powerful and rich people that are investing more than ever in it. So it's not going anywhere and it's going to be something that potentially becomes more dangerous over time because as we hand over more of these to these systems, it could potentially control and more and more of our life.
Result confidence: 0.8785989880561829
We can use however adversarial machine learning techniques to find ways to full black box Network so we can use these and we know we don't have to have perfect knowledge.
Result confidence: 0.9044815897941589
However information is powerful and the more information that we do have the more we are able to become a good GDP are based adversary. So please use GPR and let's discuss ways where we can share information. Finally, please support open source tools and research in the space because we need to keep up with where the state of the art is. So we need to keep ourselves moving and open in that way and please support ethical data companies or start one. If you come to me and you say Catherine, I'm going to charge you this much money, but I will never sell your data and I will never buy your data. I would much rather you handle my data. So I want us especially those within the EU to start a new economy around trust and privacy and ethical data youth.
Result confidence: 0.8413576483726501
Thank you very much.
Result confidence: 0.8946958780288696
Okay, we still have my horse now. I'm really gonna come up to one of the mics in the front section and raise your hand. Can I take a question from Mike one? Very interesting talk one impression that I got during the talk was with the adversary learning approach, aren't we just doing pen testing and quality assurance for the AI company's latest going to build better machines?
Result confidence: 0.9044049382209778
That's a very good question. And of course most of this research right now is coming from those companies because they're worried about this what however they've shown is. They don't really have a good way to fool to learn how to fool. This most likely they will need to use a different type of network eventually, so probably whether it's the blind spots or the linearity of these networks, they are easy to fool and they will have to come up with a different method for generating something that is robust enough to not be tricked so you to some degree. Yes, there's a cat and mouse game right, but that's why I want the research in the open source to continue as well and I would be highly suspect if they all of a sudden figure out a way to make a neural network, which has proven linear relationships that we can exploit nonlinear. And if so is usually a different type of networks as a lot more expensive to train in that doesn't actually generalize well, so we're going to
Result confidence: 0.8794476985931396
I really hate them in a way where they going to have to be more specific try harder, and I would rather do that than just kind of give up.
Result confidence: 0.9043459892272949
Hello, thank you for the nice talk. I wanted to ask have you ever tried looking at it from the other direction? Like just trying to feed the company's firstly classified data and just do it. So Matt was so massive amounts of data so that they learn from it at this point. Yes, that's these poisoning attacks. So when we talk about poisoning attacks for essentially feeding bad training data, and we're trying to get them to learn bad things or I wouldn't say bad things but we're trying to get them to learn false information and that already happens on accident all the time. So I think the more to we can if we share information and they have a publicly available API where they're actually actively learning from our information then yes, I would say poisoning is a great attack way and we can also share information of maybe how that works. So especially I would be intrigued if we can do poisoning for
Result confidence: 0.8430827260017395
Adware and malicious ad targeting. Okay. Thank you.
Result confidence: 0.9043616652488708
One more question from the internet and then we run out of time so, you know, okay. Okay. So what exactly can I do to harm my model against a treasury examples? Sorry. What exactly can I do to harm my model against adversarial example, not much what they have shown is that if you train on a mixture of real training data and adversarial data, it's a little bit harder to fool, but that just means that you have to try more iterations of adversarial input. So right now the recommendation is to train on a mixture of adversarial and real training data and to continue to do that over time and I would argue that you need to maybe do data validation on input and if you do data validation on input maybe you can recognize abnormalities, but that's because I come from mainly like production.
Result confidence: 0.8330967426300049
All of not theoretical and I think maybe she just passed things and see if they look weird. You should maybe not take them into the system.
Result confidence: 0.858120322227478
And that's all for the questions. I wish we had more time, but we just don't please give it up for Catherine terminal.
